{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder_Latent_Space.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9UNC44G4JZnD"
      ],
      "authorship_tag": "ABX9TyOBhZbPKaIoIVLD59VC2y4Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kfir1g/Engineering_final_project/blob/master/Autoencoder_Latent_Space.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4lD-CPVF-cw",
        "colab_type": "text"
      },
      "source": [
        "## Loading Tensorflow on colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJPVVwKcFy3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq7mlMr-Jtmv",
        "colab_type": "text"
      },
      "source": [
        "## Imports and Constants\n",
        "Change the following constants as you see fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JlFFJf1JyuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, InputLayer, Reshape, Conv2DTranspose\n",
        "from tensorflow.keras import Model\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import manifold\n",
        "\n",
        "TRAIN_BUF = 60000\n",
        "BATCH_SIZE = 1000\n",
        "TEST_BUF = 10000\n",
        "EPOCHES = 20\n",
        "NOISE_RATIO = 0.0    # noise should be in the range [0, 1]\n",
        "LEARNING_RATE = 1e-3\n",
        "VISUALIZE_IMAGE_EXAMPLE = True\n",
        "VISUALIZE_LATENT_SPACE = True\n",
        "SHOW_PREDICTION_EXAMPLE = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nh-uVjhJHoC",
        "colab_type": "text"
      },
      "source": [
        "## Defining the AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2OIlun4G3ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder (Model):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = Conv2D(32, 3, strides=(2, 2), activation='relu')\n",
        "        self.conv2 = Conv2D(64, 3, strides=(2, 2), activation='relu')\n",
        "        self.flatten = Flatten()\n",
        "        self.dense1 = Dense(512, activation='relu')\n",
        "        self.dense2 = Dense(10, activation='relu')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense1(x)\n",
        "        return self.dense2(x)\n",
        "\n",
        "\n",
        "class Decoder(Model):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dense1 = Dense(512, activation='relu')\n",
        "        self.dense2 = Dense(7 * 7 * 64, activation='relu')\n",
        "        self.reshape = Reshape(target_shape=(7, 7, 64))\n",
        "        self.transConv1 = Conv2DTranspose(filters=32, kernel_size=3, strides=(2, 2), activation='relu', padding='same')\n",
        "        self.transConv2 = Conv2DTranspose(filters=1, kernel_size=3, strides=(2, 2), activation='sigmoid', padding='same')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.reshape(x)\n",
        "        x = self.transConv1(x)\n",
        "        return self.transConv2(x)\n",
        "\n",
        "\n",
        "class AutoEncoder(Model):\n",
        "    def __init__(self):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def call(self, x):\n",
        "        output = []\n",
        "        x = self.encoder(x)\n",
        "        output.append(x)\n",
        "        x = self.decoder(x)\n",
        "        output.append(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UNC44G4JZnD",
        "colab_type": "text"
      },
      "source": [
        "## Loading Data\n",
        "Loading mnist data, and normalizing it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JqKbt5PJhyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_prep_data(noise_ratio=0.0):\n",
        "    \"\"\"Loads data from Mnist and normalize it to the range [0, 1]\"\"\"\n",
        "    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "    train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "    test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')\n",
        "\n",
        "    # Normalization of the data\n",
        "    train_images = (train_images - train_images.min()) / (train_images.max() - train_images.min())\n",
        "    test_images = (test_images - test_images.min()) / (test_images.max() - test_images.min()) \n",
        "\n",
        "    # Add noise to the image\n",
        "    if noise_ratio:\n",
        "        train_images = train_images + noise_ratio * np.random.normal(loc=0, scale=1.0, size=train_images.shape)\n",
        "        train_images = np.clip(train_images, 0.0, 1.0)\n",
        "        test_images = test_images + noise_ratio * np.random.normal(loc=0, scale=1.0, size=test_images.shape)\n",
        "        test_images = np.clip(test_images, 0.0, 1.0)\n",
        "        train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "        test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(TRAIN_BUF).batch(BATCH_SIZE)\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).shuffle(TEST_BUF).batch(BATCH_SIZE)\n",
        "    return train_dataset, test_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzJcwLepK-Bf",
        "colab_type": "text"
      },
      "source": [
        "## Method definition\n",
        "The following methods define the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLoGBs84K9qL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reset_states():\n",
        "    \"\"\"This function reset the accuracy and the loss\"\"\"\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "\n",
        "def loss_l2(images, predictions):\n",
        "    \"\"\"loss object\"\"\"\n",
        "    return tf.reduce_mean(tf.square(images - predictions))\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    \"\"\"Gradient decent training\"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images)[-1]\n",
        "        loss = loss_l2(images, predictions)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        train_loss(loss)\n",
        "        train_accuracy(images, predictions)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def test_step(images):\n",
        "    \"\"\"Tests the images\"\"\"\n",
        "    predictions = model(images)[-1]\n",
        "    loss = loss_l2(images, predictions)\n",
        "    test_loss(loss)\n",
        "    test_accuracy(images, predictions)\n",
        "\n",
        "    \n",
        "def train_model(train_dataset):\n",
        "    \"\"\"Train the model based on previously declared methods\"\"\"\n",
        "    for epoch in range(EPOCHES):\n",
        "        for images, labels in train_dataset:\n",
        "            train_step(images)\n",
        "        template = 'Epoch {}, Loss: {}, Accuracy: {}'\n",
        "        print(template.format(epoch + 1, train_loss.result(), train_accuracy.result() * 100))\n",
        "        print(\" -- \", flush=True)\n",
        "        reset_states()\n",
        "\n",
        "\n",
        "def visualize_latent_space(test_dataset, title=''):\n",
        "    \"\"\"\n",
        "    plots the latent space\n",
        "    :param test_dataset: The test dataset\n",
        "    :param title: the title for the figure\n",
        "    \"\"\"\n",
        "    for x, y in test_dataset:\n",
        "        latent_space = model(x)[0]\n",
        "        fitted_space = manifold.TSNE().fit_transform(latent_space)\n",
        "        for i in range(10):  # 10 possible digits\n",
        "            digit_cluster = fitted_space[np.where(y == i)]\n",
        "            plt.plot(digit_cluster[:, 0], digit_cluster[:, 1], '.')\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(title)\n",
        "        plt.legend({i: i for i in range(10)})\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "\n",
        "def visualize_image_example(dataset, noise):\n",
        "    \"\"\"Visualize an example from the training data\"\"\"\n",
        "    for images, labels in dataset:\n",
        "        image = np.reshape(images[0, :, :, :], images.shape[1:3])\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Digit={}, Noise={}%\".format(labels[0].numpy().astype(np.str), (noise*100)))\n",
        "        plt.show()\n",
        "        break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb76h3BSN8XZ",
        "colab_type": "text"
      },
      "source": [
        "## Training the Net\n",
        "This section trains the net\n",
        "There are constants regarding showing examples of the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyIlnXr4N8uQ",
        "colab_type": "code",
        "outputId": "1251feda-9b0a-4df6-a588-5b03f31e5bdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Elements for the neural network\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')\n",
        "model = AutoEncoder()\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
        "\n",
        "train_dataset, test_dataset = load_and_prep_data(NOISE_RATIO)\n",
        "if VISUALIZE_IMAGE_EXAMPLE:\n",
        "    visualize_image_example(train_dataset, NOISE_RATIO)\n",
        "train_model(train_dataset)\n",
        "if VISUALIZE_LATENT_SPACE:\n",
        "    visualize_latent_space(test_dataset, 'Latent Space with {}% noise'.format(NOISE_RATIO * 100))\n",
        "if SHOW_PREDICTION_EXAMPLE:\n",
        "    visualize_image_example(test_dataset, NOISE_RATIO) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.10920298099517822, Accuracy: 79.56458282470703\n",
            " -- \n",
            "Epoch 2, Loss: 0.06985339522361755, Accuracy: 80.19477081298828\n",
            " -- \n",
            "Epoch 3, Loss: 0.057543568313121796, Accuracy: 79.85428619384766\n",
            " -- \n",
            "Epoch 4, Loss: 0.0408923365175724, Accuracy: 79.74137878417969\n",
            " -- \n",
            "Epoch 5, Loss: 0.029925623908638954, Accuracy: 80.39830017089844\n",
            " -- \n",
            "Epoch 6, Loss: 0.02507132664322853, Accuracy: 80.6454086303711\n",
            " -- \n",
            "Epoch 7, Loss: 0.022787507623434067, Accuracy: 80.75659942626953\n",
            " -- \n",
            "Epoch 8, Loss: 0.02135458216071129, Accuracy: 80.82882690429688\n",
            " -- \n",
            "Epoch 9, Loss: 0.020249448716640472, Accuracy: 80.88214111328125\n",
            " -- \n",
            "Epoch 10, Loss: 0.019458089023828506, Accuracy: 80.9204330444336\n",
            " -- \n",
            "Epoch 11, Loss: 0.01879369281232357, Accuracy: 80.95463562011719\n",
            " -- \n",
            "Epoch 12, Loss: 0.018252993002533913, Accuracy: 80.97989654541016\n",
            " -- \n",
            "Epoch 13, Loss: 0.017774153500795364, Accuracy: 81.0042495727539\n",
            " -- \n",
            "Epoch 14, Loss: 0.01735217683017254, Accuracy: 81.02483367919922\n",
            " -- \n",
            "Epoch 15, Loss: 0.017070969566702843, Accuracy: 81.04112243652344\n",
            " -- \n",
            "Epoch 16, Loss: 0.01673022098839283, Accuracy: 81.05685424804688\n",
            " -- \n",
            "Epoch 17, Loss: 0.01642482914030552, Accuracy: 81.07341766357422\n",
            " -- \n",
            "Epoch 18, Loss: 0.016170425340533257, Accuracy: 81.08529663085938\n",
            " -- \n",
            "Epoch 19, Loss: 0.015926115214824677, Accuracy: 81.09673309326172\n",
            " -- \n",
            "Epoch 20, Loss: 0.01572365127503872, Accuracy: 81.10707092285156\n",
            " -- \n",
            "Epoch 21, Loss: 0.0155179463326931, Accuracy: 81.11709594726562\n",
            " -- \n",
            "Epoch 22, Loss: 0.01532230619341135, Accuracy: 81.12671661376953\n",
            " -- \n",
            "Epoch 23, Loss: 0.015120601281523705, Accuracy: 81.13529205322266\n",
            " -- \n",
            "Epoch 24, Loss: 0.014965374954044819, Accuracy: 81.14334106445312\n",
            " -- \n",
            "Epoch 25, Loss: 0.01484578475356102, Accuracy: 81.14910125732422\n",
            " -- \n",
            "Epoch 26, Loss: 0.014685576781630516, Accuracy: 81.15747833251953\n",
            " -- \n",
            "Epoch 27, Loss: 0.014577698893845081, Accuracy: 81.16156005859375\n",
            " -- \n",
            "Epoch 28, Loss: 0.01444100122898817, Accuracy: 81.16741943359375\n",
            " -- \n",
            "Epoch 29, Loss: 0.014326279051601887, Accuracy: 81.17350006103516\n",
            " -- \n",
            "Epoch 30, Loss: 0.01421274058520794, Accuracy: 81.17798614501953\n",
            " -- \n",
            "Epoch 31, Loss: 0.014100524596869946, Accuracy: 81.1841812133789\n",
            " -- \n",
            "Epoch 32, Loss: 0.01400983426719904, Accuracy: 81.18791198730469\n",
            " -- \n",
            "Epoch 33, Loss: 0.01391573715955019, Accuracy: 81.19165802001953\n",
            " -- \n",
            "Epoch 34, Loss: 0.013772404752671719, Accuracy: 81.19815826416016\n",
            " -- \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}